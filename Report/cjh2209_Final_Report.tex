\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Applications of Speech Recognition Methods to Automatic Music Transcription\\
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
%should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Connor Hargus}
\IEEEauthorblockA{\textit{UNI: cjh2209} \\
\textit{Fundamentals of Speech Recognition - Beigi}\\
}
}

\maketitle

\begin{abstract}
This paper describes an AMT system based on technologies and principles from speech recognition. Using a database of MIDI score files drawn from scales and jazz solo transcriptions, this system generates saxophone recordings. Using a time-delay neural network trained to identify pitches based on MFCCs extracted from the recording, the system outputs probabilities of note occurrences for each frame. These are then fed into a language model using an HMM, which boosts accuracy by accounting for noisy measurements and the relative likelihoods of various sequences of pitches. In the monophonic case, the system achieves 97.7\% accuracy on all frames without the language model and with 98.1\% accuracy with it. Future work remains to be done both to transcribe polyphonic recordings and to integrate the neural network with a more sophisticated language model attuned to the syntax, relative ordering of notes, and grammatical structure distinguishing music from speech.
\end{abstract}

\section{Introduction}

The transcription of music from recording to paper is a painstaking task largely still performed by hand by expert musicians. Despite the importance of having music in score form in order to analyze the musical theory of a piece or have a musical group play it, modern technologies which perform automatic music transcription (AMT)  are still largely incapable of extracting pitches from audio recordings due to the complex combination of timbres which arise from multiple and varied instruments playing at once. In this paper, I will introduce some promising research which applies some of the core technologies currently in use in speech recognition to the problem of AMT. 

Historically, most AMT systems have relied on the crude method of attempting to identify the fundamental frequency for a given note in a recording. While this method works perfectly well in the monophonic case (a single instrument playing a single note at a time), it cannot possible identify two lines of music simultaneously as in the case of polyphonic music. In order to combat this, many past systems have relied on unique spectral composition of a particular controlled instrument. By finding the constituent notes whose compositions might combine to form the observed recording, these methods have achieved some success. However, these results often only work under highly controlled conditions and tend to generalize poorly to real-world musical recordings. A notable decrease in the pace of improvement in AMT technology was observed in the early 2010s, and in 2012 Humphrey and Bello suggested that convolutional networks might be a more promising path forward in chord identification. Sigtia, Benetos, and Dixon built upon this model in February 2016 with their contribution of a convolutional neural network for polyphonic AMT. These recent models have given renewed optimism to those seeking an AMT system capable of outperforming human experts.

Abandoning the antiquated method of identifying fundamental frequencies or template matching for particular timbral frequencies, my research utilizes a time-delay neural network to extract the relevant pitch information from Mel-frequency cepstral coefficients (MFCCs) and a hidden markov model (HMM) to attempt to capture the language aspect of music. The hope with such models is that they will rid us of the need to try to tweak parameters to quantify exactly what constitutes a note, but rather that an artificial neural network can decide which features and combinations thereof are significant in the identification of pitches and in the distinguishing between and decomposing of pitches in polyphonic case.

\section{Data}

For my dataset, I chose to utilize a combination of MAPS (MIDI Aligned Piano Sounds) along with a set of jazz solos from the Weimar database. MAPS contains tracks with chromatic scales and trills covering the span of MIDI pitches, and I figured it would provide a good baseline for training the deep neural network as well as giving small transition probabilities to adjacent notes. The Weimar database, meanwhile, contains a set of over 500 famous jazz solos transcribed to MIDI format. The fact that this database contained solos was crucial to my aim of developing a language model which not only detects individual notes but identifies them based on their surrounding context. As in speech, the language of music follows rules concerning intervals, patterns, chord progressions, and much more which can be used to better identify notes. Since the Weimar database provided solos, I could thus isolate a single line of music and predict the likelihoods of transitions between notes. 

Additionally, the fact that both databases contain MIDI scores meant that I had all the information, including pitch, velocity, and precise timing, to generate new audio recordings. In order to do so, I used a software synthesizer called FluidSynth. FluidSynth has much flexibility and allows the user to choose ``soundfonts'' corresponding with different instrument timbres. Seeking an instrument which is commonly used in monophonic recordings and which has an amplitude envelope (not something so simple as a sine wave), I settled on a saxophone soundfont. Using the midi2audio Python package, then, I was able to generate the FluidSynth sound recordings for each monophonic MIDI score from my databases. 

While the Weimar database does not include recordings of its source material, the MAPS database contains a set of real-world recordings generated from the MIDI files directly on a Disklavier self-playing piano. Testing the neural network's behavior on real-world recordings, as opposed to synth-generated recordings alone, is thus possible. The MAPS database is also convenient for future development because it includes a set of polyphonic classical score files and recordings with which a polyphonic AMT system might also be tested.

\section{Approach with Kaldi}

The strategy I originally envisioned in approaching this research project was to apply an existing speech recognition toolkit (Kaldi) directly to the problem of automatic music transcription. Rather than having speech phones as the outputs of a neural network, I planned to train my neural network to output probabilities of the presence of particular notes. The score files in my databases span a range of 88 notes, so I used these 88 notes as my set of phones, which in the context of ASR were interchangeable with the words themselves. Kaldi utilizes an HCLG graph to decode for each layer of context-dependent phones, phones, words, and sequences of words the most likely sequence. In the application of AMT, I treated phones and words as equivalent so the layers of the graph which decode the relationships between phones and words would in this case be degenerate and rather trivial.

Building off the Tedlium recipe for speech recognition using Kaldi, I began by generating all of the prerequisite files called for by the recipe. These include \texttt{.stm} files for each recording, containing line by line descriptions of the notes. The MIDI files from my datasets--from which I found I could extract time and pitch information using Python packages Python-Midi and mido--contain the exact locations of the notes, which I at first figured would confer an advantage in Kaldi over traditional speech databases with only end timestamps for long utterances. However, it soon became apparent that the process of alignment in Kaldi depends on having longer utterances to align, and given that some of my notes were under 0.1 seconds I found that Kaldi had trouble finding any alignment at all despite it being aligned to begin with (discussed in more depth in following paragraphs). Creating the \texttt{.sph} sound files was a simple matter of using the \texttt{sox} sound processing command line program for converting my FluidSynth \texttt{.wav} files to \texttt{.sph}. I created a corpus file (\texttt{corpus.en.gz}) by stringing notes together in the order of their appearance in my score files. Finally, I constructed the dictionary file as a simple one-to-one mapping of the MIDI note numbers present in my database (21 through 108). This would then be used to create a language model attempting to capture the language of runs, intervals, and patterns in the jazz solo database.

Having built the database folder for the Tedlium recipe (\texttt{db}), I was then able to run the recipe stages 2 through 4 to prepare the data, dictionary, and language model. In stage 5, which trains the language model, I ran into an issue wherein Kaldi expects a much larger corpus with a larger pool of n-grams to draw on for computing the transition probabilities. Where a typical speech corpus like Kaldi might produce on the order of millions or tens of millions of n-grams, my databases yielded only around 40,000. However, given that my language (musical notes) only has 88 words, many fewer than a spoken language, I decided that my lower number of n-grams would suffice in the computation of transition probabilities and thus dramatically reduced the required number of n-grams for the language model training process.

After initially trying to train the monophones on utterances which contained only the notes themselves (using the start and end points of each midi note), I found that the Kaldi's MFCC extraction process is performed on each utterance separately and this caused issues since some of my notes were so short that the available window of audio was too short and thus had too few samples to compute MFCCs. I tried to solve this by reconstructing the database to ignore notes in my score files with very short durations. However, this was undesirable since I wanted my recognition system to be able to detect even short notes. I next rebuilt the database to contain longer utterances of about five seconds in duration and containing multiple notes. This, I figured, more closely aligned with the length and format of utterances input in the original Tedlium dataset. Indeed, with these longer utterances I was able to extract the MFCCs successfully. However, I felt it a shame to simply throw out the precise alignment information on a note-by-note basis which was provided in the MIDI files of my databases. Getting accurate results from this process would now depend on the ability to correctly align the notes just as phones are typically aligned with the MFCCs in the Kaldi training process.

With stage 7's MFCC extraction complete, I lowered the requisite number of utterances for training the monophone model  in order to be compatible with the size of my datasets. While the monophone training completed, the resulting model proved incapable of aligning properly in stage 10 so that Kaldi's \texttt{steps/train\_deltas.sh} script could successfully run. At first I assumed this failure was due to the low number of phones present in my dataset at the extremity of my note range. In particular, the middle register of around notes 40 to 50 accounts for a much larger percentage of the notes since most of the jazz solos in the Weimar database tended to be in this range. I thus added duplicates of the chromatic scales from the MAPS database (which cover every note in the range) to the dataset in the hopes improving accuracy on those notes. However, while this significantly reduced the number of warnings offered by Kaldi, it did not resolve the eventual failure in the alignment phase.

Ultimately, I was unable to successfully align the monophones and triphones in Kaldi and proceed to decoding. I think there were a combination of factors which gave me such trouble in this stage, and I learned quite a bit about the workings of Kaldi in the process of trying to address them. Perhaps the largest issue I was unable to resolve was that of note and rest duration. Because of the high variation in the rhythm of music, I suspect that certain reasonable assumptions about word and silence duration which hold in speech break down in music. In particular, the quick succession of notes in my jazz solo database and speed of trills in the MAPS database likely caused Kaldi to be unable to converge on the correct alignment of monophones to MFCCs because there is so much variation from one recording to the next in the duration of the phones. The length of silence (or rests in music) is also a much more important feature in music than speech recognition, and it is possible that voice activity detection was preventing a model from being trained on the silence phone (although I was unable to locate any scripts where this would be performed in the MFCC extraction rather than the ivector computation). Another potential source of problems is the fact that a saxophone produces a sharp attack at the beginning of each note. This creates an inconsistency across the lifespan of a note which I fear a simple monophone model might not have been able to capture. Having delved into Kaldi's rich set of scripts, I was also routinely been confronted by the fact that Kaldi was designed with larger vocabularies, corpora, and probably most importantly datasets in mind. I concluded that for the purposes of this project it felt too much like I was forcing a square peg into a round hole. While I would like to reintegrate what I worked on later with Kaldi and OpenFST (see next steps), I decided that the constant rebuilding of the database and re-uploading to Google cloud was becoming a hindrance to making real progress. It was with these realizations in mind that I decided to develop my own model using only those principles of ASR which were necessary to my project, where I would be responsible for all of the design decisions and be able to better tailor the program to the task of AMT. Creating my own AMT system would also allow me to take advantage of the fact that my datasets had note by note information, making the task of alignment which the Tedlium recipe demands completely unnecessary.


\section{Approach with Keras and Hmmlearn}

Simultaneously with my attempt in Kaldi, I was also working on building a time-delay neural network in Keras in order to develop a deeper understanding of the workings of Kaldi's TDNN. Prior to this class, I had had no experience working with neural networks. I thus figured that building my own time-delay neural network would give me some insights into why the Tedlium recipe's TDNN is structured as it is. Keras is a high-level neural network library using Python as an interface and TensorFlow for the backend. Keras and TensorFlow have become something of the industry standards as of late for building artificial neural networks, and I figured that the abundance of documentation on them would allow me to make much swifter progress in my understanding of the inner workings of neural networks. As my progress got stuck in the alignment and decoding phases of training in Kaldi, I gradually shifted over the bulk of my project to Keras for the TDNN and hmmlearn for the language model, as is described in more detail in the following description of my work.

In setting up my own project, I was able to use the same audio files as had been generated by FluidSynth for Kaldi training (the \texttt{.wav} files prior to being converted to \texttt{.sph}). After splitting the MIDI and audio data into training and test set randomly, I used a Python library called python\_speech\_features to extract the MFCCs from the audio files and store them in a NumPy array. In line with what i had learned from the neural network's usage of MFCCs in the Tedlium example, I decided to generate 13 MFCCs for each frame of audio using a 25ms window with a 10ms gap between windows. For each frame, I assembled an array of the 30 surrounding MFCCs in order for the TDNN to make use of convolutions along the dimension of time. Meanwhile, my system simultaneously extracts the expected output note for a given frame using the associated MIDI file. By computing the length of the audio recording and dividing this by the frame rate for the MFCCs, my program locates whichever notes are sounding during each frame and write a 1 to that note's column for the given frame row. Importantly, after computing all the notes which are on, my program then finds all frame rows for which all columns are 0 and then supplies a one in a new column representing whether a frame is silent. Because I had explicitly generated MFCCs for the entire audio files, doing this myself made me confident that voice activity detection was not suppressing these silence (or rest) features as it may have been with Kaldi. Since there is a range of 88 notes spanned by the score files in my data sets, I thus had a total of 89 output phones I wanted to train on when including the silence phone.

With MFCCs and expected output notes computed for every frame of my audio files, the next step was to create a TDNN which would map from one to the other. While I had no experience with neural networks prior to this class, documentation online and discussion in class about convolutional neural networks allowed me piece together how I could create a convolutional neural network along the time dimension (essentially a time-delay neural network). Built to resemble the neural network utilized by the Tedlium recipe, my neural network combines a series of convolutional filters which gradually broaden in scope from close neighboring features to larger windows in time. I then further modified the number of layers and size of convolutions empirically through multiple training sessions before settling on the final neural network in my submission. Specifically, the TDNN I built takes in 30 MFCCs arranged in chronological order and begins by feeding them into a one-dimensional convolutional layer with 64 features which looks at a window size of 2. After another convolutional layer with window size of 2 and 64 features, max pooling is performed as is common in CNN architecture. Next, another two layers of one-dimensional convolution occur, this time with a larger window size of 3. After another layer of max pooling, another layer of convolution is performed with a window size of 4 before its outputs feed into a fully-connected final layer. Note that each of these layers uses a rectified linear unit (ReLU) activation function. Also, between each of the previous pairs of layers I included dropout layer with 0.2, 0.2, and 0.5 rates of dropout, respectively. As we discussed in class, dropout layers help to prevent a neural network from overfitting by forcing neurons to work well generally on the task rather than having specific neurons find very specific features. This is useful in identifying the larger and more important features recognized throughout the network rather than becoming over-reliant on a small subset of the neurons. Finally, the last layer of the network is another dense layer which uses a softmax activation function to assign output probabilities to each of the notes, of which there are 89 including silence as previously discussed.

Utilizing this neural network directly by simply taking the maximum of the probabilities for each frame, I found a high accuracy rating of about 97.8\%. This is about as good as I had hoped for, given that my recordings were monophonic and played by only a single instrumental synth. Upon closer inspection, however, it was clear that a language model would be necessary even with such high accuracies. Once again using mido, the Python library which I had used for detecting the notes for particular frames from the MIDI file, I created a function to recreate a new MIDI file based on these highest-probability outputs from the TDNN. Listening to the resulting recordings, I noticed occasional glitches where it seemed the neural net had, for a single frame in the middle of a held note, been confused and selected a different note instead. Since this different note could be in a different octave entirely from the note that should have been held, and because they would restart the initial attack on the held note, a noticeable disruption would be caused. Wanting to make better use of the contextual information of the output probabilities from the neural network in sequence over time, I decided that a language model would be required. I therefore built a hidden markov model (HMM) using a Python library called hmmlearn, which was originally included in scikit-learn but branched off into an independent project in recent years. 

Using the frame to frame transitions between notes as my hidden variable transitions, I hoped to mimic musical lines in a monophonic recording. I further decided to model the probabilities output by the time-delay neural network for each frame as a multivariate Gaussian emission for a given underlying state (note). To train the HMM, I used the training datasets note sequences to compute the the maximum likelihood predictions for the start probabilities, transition probabilities, and observable emission probabilities (with Guassian means and covariances). The decoding process for an unseen sequence of TDNN output probabilities is then handled by the Viterbi algorithm.

\section{Results}

My model evaluates itself both on per-frame accuracy of predicting the pitch (or silence) based on an MFCC, as well as on the word error rate (WER) metric commonly used in speech recognition. The results provided below demonstrate the capacity of TDNNs to learn pitch information based on MFCC features, as well as for an HMM to function as a language model providing large improvements to the WER. With just the TDNN, taking the maximum pitch probability from the softmax layer, the accuracy on a per-frame basis was 97.7\% for the test data. The per-frame accuracy with the added language model HMM made a slight improvement on this already strikingly high accuracy to 98.1\%. We note however, that while the TDNN was impressively accurate at the frame level, the mistakes that it did make were glaring in the reconstructed audio. For instance, the TDNN would occasionally output the wrong pitch for a single frame within a long held note. While it may be only a single frame, the reconstructed audio would have a noticeable glitch at the point. This is because MIDI notes have an amplitude envelope which has a strong initial attack. With a single frame out of place in a held note, the correct note will experience a second attack in the middle which is not true to the original recording. Another problematic area for the TDNN, as visible in Figure \ref{fig:midipics}, is at the beginning and end of notes. I suspect that due to the sharp attack of the saxphone at the beginning of the note, the MFCCs are less predictable at that instant and thus the TDNN is more liable to make mistakes. As visible in Figure \ref{fig:midipics}, these mistakes tend to be one or two pitches off, but they still create audible artifacts in the reconstructed audio.

By adding a language model based on the transition probabilities among notes (and silence) in the jazz solos of the Weimar database, we are able to remove such artifacts from the reconstructed MIDI files 

\begin{figure}[!t]
\centering
\begin{subfigure}[b]{0.45\textwidth}
   \includegraphics[width=3.3in]{images/SoWhat_OG}
   \caption{Original MIDI file}
   \label{fig:Ng1} 
\end{subfigure}

\begin{subfigure}[b]{0.45\textwidth}
   \includegraphics[width=3.3in]{images/SoWhat_TDNN}
   \caption{Reconstructed MIDI file using TDNN max probability}
   \label{fig:Ng1} 
\end{subfigure}

\begin{subfigure}[b]{0.45\textwidth}
   \includegraphics[width=3.3in]{images/SoWhat_HMM}
   \caption{Reconstructed MIDI file using HMM}
   \label{fig:Ng2}
\end{subfigure}

\caption{Reconstructions of original MIDI file with and without a language model for beginning of John Coltrane's solo on ``So What''. Rows of the grid are notes, and time moves along the x-axis (note that the reconstructed files have different tempo grid lines for time, as my model does not currently extract tempo information. Notice the noise at the beginning and end of some notes in figure (b) is removed using the language model, with figures (a) and (c) being nearly identical.}

\label{fig:midipics}
\end{figure}

%\includegraphics[width=3.5in]{images/SoWhat_HMM}
%\caption{Simulation results for the network.}
%\label{fig_sim}

\section{Next Steps}

With this model as a basis, it seems clear that there are many potential avenues for exploration in further research. The most obvious of these would be polyphonic AMT, for which the TDNN presented here could easily be adapted using the polyphonic classical score files in the MAPS database. Moving to polyphonic music will, however, demand a new language model up to the task of representing multiple musical lines moving in different directions at once. A single hidden markov model would likely not be enough, unless each possible combination of notes is treated as a separate ``word'' in the language of speech recognition. However, this would likely require a much larger dataset to get accurate results on this exponentially large number of note combinations. One promising way to reduce this complexity could be to take advantage of the relative nature of pitches. For the purposes of this research, my TDNN does not assume any relative ordering of pitches. This relative nature is currently something which must be learned by the model, but it seems likely that a better neural network architecture could explicitly take advantage of this prior knowledge.

In addition to the relative ordering of pitches, another crucial piece of prior knowledge currently omitted from my model is the role of duration in music. While the variety of duration of notes may have been a hindrance in the alignment stage of Kaldi training, we might take advantage of the rhythmic nature of music. One possible option which I wanted to include but did not have time to implement is a better duration model. In the paper ``Neural Network Phone Duration Model for Speech Recognition'', researcher Tanel Alumae describes the a model in which a neural net is trained on a set of sequences to create a probability density function for a phone assigning different probabilities to different durations. I suspect that a language model of music would benefit dramatically from such a duration model given that notes are typically released in time with the music and do not follow the simple geometric model assumed by an HMM.

\begin{thebibliography}{99} % Bibliography - this is intentionally simple in this template

\bibitem{benetos}
Benetos, E., Dixon, S., Giannoulis, D., Kirchhoff, H., \& Klapuri, A. (2013). Automatic music transcription: challenges and future directions. \textit{Journal of Intelligent Information Systems}, 41(3), 407-434.

\bibitem{benetos2}
Benetos, E., \& Dixon, S. (2012). A Shift-Invariant Latent Variable Model for Automatic Music Transcription. Computer Music Journal, 36(4), 81-94.

\bibitem{doege}
Doege, M. C. (2017, October 15). PySynth. Retrieved October 15, 2017, from https://github.com/mdoege/PySynth

\bibitem{humphrey}
Humphrey, E. J., \& Bello, J. P. (2012). Rethinking Automatic Chord Recognition with Convolutional Neural Networks. 2012 11th International Conference on Machine Learning and Applications. 

\bibitem{sigtia}
Sigtia, S., Benetos, E., \& Dixon, S. (2016). An End-to-End Neural Network for Polyphonic Piano Music Transcription. IEEE/ACM \textit{Transactions on Audio, Speech, and Language Processing}, 24(5), 927-939.

\bibitem{waibel}
Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., \& Lang, K. J. (1990). Phoneme Recognition Using Time-Delay Neural Networks. Readings in Speech Recognition, 393-404.
 
\end{thebibliography}


\end{document}
